# Inference Configuration for Phase 7: Real-Time Non-Blocking Detection

inference:
  async:
    max_workers: 4
    batch_size: 32
    timeout: 5.0
  
  optimization:
    quantization: true
    torchscript: false
  
  rate_limiting:
    enabled: true
    max_requests_per_second: 100
    per_ip: false  # Set to true for per-IP rate limiting
    max_ips: 10000
  
  queue:
    enabled: false  # Enable RequestQueueManager for advanced queuing
    max_size: 1000
    batch_timeout: 0.1  # seconds
    batch_size: 32
  
  metrics:
    collect: true
    update_interval: 60  # seconds
  
  device:
    auto_detect: true  # Auto-detect CUDA if available
    force_cpu: false
    force_cuda: false
