{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAF Model Training - DistilBERT Fine-tuning\n",
    "\n",
    "This notebook fine-tunes DistilBERT for Web Application Firewall (WAF) attack detection.\n",
    "\n",
    "**Dataset:** notesbymuneeb/ai-waf-dataset from HuggingFace\n",
    "\n",
    "**Model:** distilbert-base-uncased â†’ binary classifier (benign/malicious)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's enable GPU and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these parameters as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16  # Increase to 32 if GPU has enough memory\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 512\n",
    "OUTPUT_DIR = \"./waf-distilbert\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WAF dataset from HuggingFace\n",
    "print(\"Loading dataset: notesbymuneeb/ai-waf-dataset\")\n",
    "dataset = load_dataset(\"notesbymuneeb/ai-waf-dataset\")\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "print(\"Sample entries:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Text: {dataset['train'][i]['text'][:200]}...\")\n",
    "    print(f\"Label: {dataset['train'][i]['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to integers\n",
    "label_map = {\"benign\": 0, \"malicious\": 1}\n",
    "\n",
    "def map_labels(example):\n",
    "    example[\"label\"] = label_map[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(map_labels)\n",
    "\n",
    "# Cast label to ClassLabel for stratification\n",
    "dataset = dataset.cast_column(\"label\", ClassLabel(names=[\"benign\", \"malicious\"]))\n",
    "\n",
    "# Split into train/val (90/10)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42, stratify_by_column=\"label\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Eval size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"benign\", 1: \"malicious\"},\n",
    "    label2id={\"benign\": 0, \"malicious\": 1},\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=False,  # Dynamic padding via DataCollator\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f\"Tokenization complete!\")\n",
    "print(f\"Train features: {train_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics computation\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy, precision, recall, F1.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision for faster training\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This will take some time depending on dataset size and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*50)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Final Evaluation Results\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "for key, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "print(f\"Saving model to {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"\\nModel saved successfully!\")\n",
    "!ls -la {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Model\n",
    "\n",
    "Zip and download the trained model to use locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model directory\n",
    "!zip -r waf-distilbert-model.zip {OUTPUT_DIR}\n",
    "\n",
    "# Download (in Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('waf-distilbert-model.zip')\n",
    "    print(\"\\nDownload started! Check your browser downloads.\")\n",
    "except ImportError:\n",
    "    print(\"\\nNot running in Colab. Model saved to: waf-distilbert-model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model (Optional)\n",
    "\n",
    "Quick test to verify the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=OUTPUT_DIR, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Test samples\n",
    "test_samples = [\n",
    "    \"GET /api/users HTTP/1.1\",\n",
    "    \"GET /search?q=<script>alert('xss')</script> HTTP/1.1\",\n",
    "    \"POST /login username=admin&password=' OR '1'='1 HTTP/1.1\",\n",
    "    \"GET /products/123 HTTP/1.1\",\n",
    "    \"GET /admin/../../../etc/passwd HTTP/1.1\",\n",
    "]\n",
    "\n",
    "print(\"\\nTest Predictions:\")\n",
    "print(\"=\"*60)\n",
    "for sample in test_samples:\n",
    "    result = classifier(sample)[0]\n",
    "    print(f\"\\nInput: {sample[:50]}...\" if len(sample) > 50 else f\"\\nInput: {sample}\")\n",
    "    print(f\"Prediction: {result['label']} (confidence: {result['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Push to HuggingFace Hub\n",
    "\n",
    "If you want to share your model on HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you want to push to HuggingFace Hub\n",
    "# from huggingface_hub import login\n",
    "# login()  # Enter your HuggingFace token\n",
    "\n",
    "# trainer.push_to_hub(\"your-username/waf-distilbert\")\n",
    "# tokenizer.push_to_hub(\"your-username/waf-distilbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "After downloading the model:\n",
    "\n",
    "1. Extract `waf-distilbert-model.zip` to your project's `models/waf-distilbert` directory\n",
    "\n",
    "2. Load the model in your application:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "model_path = \"models/waf-distilbert\"\n",
    "classifier = pipeline(\"text-classification\", model=model_path)\n",
    "\n",
    "result = classifier(\"GET /api/users HTTP/1.1\")\n",
    "print(result)  # [{'label': 'benign', 'score': 0.99}]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Training complete!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
